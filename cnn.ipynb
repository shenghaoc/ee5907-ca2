{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shenghaoc/ee5907-ca2/blob/main/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_SUBJECTS = 68\n",
    "NUM_CHOSEN = 25\n",
    "NUM_IMAGES_PER_SUBJECT = 170\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "NUM_IMAGES = NUM_CHOSEN * NUM_IMAGES_PER_SUBJECT\n",
    "NUM_TRAIN_IMAGES_PER_SUBJECT = np.int_(np.around(TRAIN_RATIO * NUM_IMAGES_PER_SUBJECT))\n",
    "NUM_TRAIN_IMAGES = NUM_CHOSEN * NUM_TRAIN_IMAGES_PER_SUBJECT\n",
    "NUM_TEST_IMAGES = NUM_IMAGES - NUM_TRAIN_IMAGES\n",
    "\n",
    "NUM_SELFIES = 10\n",
    "NUM_TRAIN_SELFIES = np.int_(np.around(TRAIN_RATIO * NUM_SELFIES))\n",
    "NUM_TEST_SELFIES = NUM_SELFIES - NUM_TRAIN_SELFIES\n",
    "SELFIE_LABEL = NUM_SUBJECTS + 1\n",
    "\n",
    "NUM_TOTAL_TRAIN_IMAGES = NUM_TRAIN_IMAGES + NUM_TRAIN_SELFIES\n",
    "NUM_TOTAL_TEST_IMAGES = NUM_TEST_IMAGES + NUM_TEST_SELFIES\n",
    "\n",
    "SEED1 = 2021\n",
    "SEED2 = 2022\n",
    "\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "NUM_PIXELS = WIDTH * HEIGHT\n",
    "\n",
    "# New constants due to need to fit input for tensorflow\n",
    "NUM_PEOPLE = NUM_CHOSEN + 1  # meaning plus the person with 10 selfies\n",
    "NUM_CHANNELS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure that the directory to store figures is created\n",
    "figures_directory = Path(\"report\") / \"figures\"\n",
    "figures_directory.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Must start from 1 to accommodate folder naming scheme\n",
    "# Choose NUM_CHOSEN elements from NUM_SUBJECTS integers without replacement\n",
    "chosen = np.random.default_rng(SEED1).choice(\n",
    "    np.arange(1, NUM_SUBJECTS + 1), NUM_CHOSEN, replace=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load images from disk\n",
    "# Use lists for manual looping without use of numpy functions\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Assume PIE is in pwd\n",
    "directory = Path(\"PIE\")\n",
    "for i in range(len(chosen)):\n",
    "    # Do not flatten yet, need to split train and test for each subject\n",
    "    subject_images = []\n",
    "    subject_labels = []\n",
    "    subdirectory = directory / str(chosen[i])\n",
    "    # Order is arbitrary for glob, but better to shuffle anyway\n",
    "    files = list(subdirectory.glob(\"*.jpg\"))\n",
    "    np.random.default_rng(SEED2).shuffle(files)\n",
    "    for filename in files:\n",
    "        # PIL is slower but OpenCV is unnecessary\n",
    "        im = Image.open(filename)\n",
    "        subject_images.append(np.array(im))\n",
    "        # For tensorflow input, use sequential label\n",
    "        subject_labels.append(i)\n",
    "    images.append(subject_images)\n",
    "    labels.append(subject_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Slightly altered code for selfies\n",
    "selfie_images = []\n",
    "selfie_labels = []\n",
    "\n",
    "directory = Path(\"resized\")\n",
    "# Assume selfies have been resized and folder is in pwd\n",
    "for filename in directory.glob(\"*.jpg\"):\n",
    "    im = Image.open(filename)\n",
    "    selfie_images.append(np.array(im))\n",
    "    # For tensorflow input, use number of chosen subjects (25) to avoid clashes\n",
    "    selfie_labels.append(NUM_CHOSEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Further processing without disk access\n",
    "# Train-test split\n",
    "images_train, images_test = np.split(\n",
    "    np.array(images), [NUM_TRAIN_IMAGES_PER_SUBJECT], axis=1\n",
    ")\n",
    "labels_train, labels_test = np.split(\n",
    "    np.array(labels), [NUM_TRAIN_IMAGES_PER_SUBJECT], axis=1\n",
    ")\n",
    "\n",
    "selfie_images_train, selfie_images_test = np.split(\n",
    "    np.array(selfie_images), [NUM_TRAIN_SELFIES]\n",
    ")\n",
    "selfie_labels_train, selfie_labels_test = np.split(\n",
    "    np.array(selfie_labels), [NUM_TRAIN_SELFIES]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Flatterning\n",
    "# For Conv2D, a 4+D tensor is required, add 1 for the grayscale channel\n",
    "images_train = images_train.reshape(NUM_TRAIN_IMAGES, WIDTH, HEIGHT, NUM_CHANNELS)\n",
    "selfie_images_train = selfie_images_train.reshape(\n",
    "    NUM_TRAIN_SELFIES, WIDTH, HEIGHT, NUM_CHANNELS\n",
    ")\n",
    "images_test = images_test.reshape(NUM_TEST_IMAGES, WIDTH, HEIGHT, NUM_CHANNELS)\n",
    "selfie_images_test = selfie_images_test.reshape(\n",
    "    NUM_TEST_SELFIES, WIDTH, HEIGHT, NUM_CHANNELS\n",
    ")\n",
    "\n",
    "labels_train = labels_train.reshape(NUM_TRAIN_IMAGES)\n",
    "labels_test = labels_test.reshape(NUM_TEST_IMAGES)\n",
    "\n",
    "# Combine PIE images and selfies\n",
    "total_images_train = np.append(\n",
    "    images_train,\n",
    "    selfie_images_train,\n",
    "    axis=0,\n",
    ")\n",
    "total_labels_train = np.append(labels_train, selfie_labels_train)\n",
    "\n",
    "total_images_test = np.append(\n",
    "    images_test,\n",
    "    selfie_images_test,\n",
    "    axis=0,\n",
    ")\n",
    "total_labels_test = np.append(labels_test, selfie_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start of CNN code\n",
    "import tensorflow as tf\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "CONV_KERNEL_SIZE = 5\n",
    "MAX_POOL_KERNEL_SIZE = 2\n",
    "MAX_POOL_SIZE = 2\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numpy arrays\n",
    "# Use built-in one-hot encoder, the numerical labels have no meaning, encoding is necessary to avoid misinterpretation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (total_images_train, tf.keras.utils.to_categorical(total_labels_train))\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (total_images_test, tf.keras.utils.to_categorical(total_labels_test))\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.cache()\n",
    "    .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        # Not really necessary, but good practice?\n",
    "        tf.keras.layers.Rescaling(1.0 / 255, input_shape=(WIDTH, HEIGHT, NUM_CHANNELS)),\n",
    "        tf.keras.layers.Conv2D(20, CONV_KERNEL_SIZE, activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(MAX_POOL_KERNEL_SIZE, MAX_POOL_KERNEL_SIZE),\n",
    "            strides=(MAX_POOL_SIZE, MAX_POOL_SIZE),\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(50, CONV_KERNEL_SIZE, activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(MAX_POOL_KERNEL_SIZE, MAX_POOL_KERNEL_SIZE),\n",
    "            strides=(MAX_POOL_SIZE, MAX_POOL_SIZE),\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),  # too many dimensions after Conv2D\n",
    "        tf.keras.layers.Dense(500, activation=\"relu\"),\n",
    "        # Keras documentation: often used for last layer because result can be interpreted as\n",
    "        # a probability distribution\n",
    "        tf.keras.layers.Dense(NUM_PEOPLE, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(amsgrad=True),  # newest ADAM\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),  # multi-class labeling\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 20)        520       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 20)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 50)        25050     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1250)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               625500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                13026     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 664,096\n",
      "Trainable params: 664,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 4s 14ms/step - loss: 3.3264 - accuracy: 0.0141\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 3.2608 - accuracy: 0.0382\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2590 - accuracy: 0.0117\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2591 - accuracy: 0.0386\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 3.2600 - accuracy: 0.0027\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2560 - accuracy: 0.0423\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 3.2548 - accuracy: 0.0402\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2537 - accuracy: 0.0402\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2518 - accuracy: 0.0416\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2832 - accuracy: 0.0798\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 3.2514 - accuracy: 0.0406\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 3.2604 - accuracy: 0.0322\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2480 - accuracy: 0.0406\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 3.2416 - accuracy: 0.0423\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2862 - accuracy: 0.0399\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2617 - accuracy: 0.0443\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2907 - accuracy: 0.0597\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 3.2380 - accuracy: 0.0282\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2544 - accuracy: 0.0510\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2389 - accuracy: 0.0486\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2328 - accuracy: 0.0493\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.2101 - accuracy: 0.0426\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.1711 - accuracy: 0.0607\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.1480 - accuracy: 0.0533\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 3.0861 - accuracy: 0.0775\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 3.0457 - accuracy: 0.0630\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 2.9509 - accuracy: 0.1009\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 2.8713 - accuracy: 0.1211\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.7191 - accuracy: 0.1700\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 2.5879 - accuracy: 0.2176\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.5837 - accuracy: 0.2166\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.6375 - accuracy: 0.1744\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.3345 - accuracy: 0.2602\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 2.4747 - accuracy: 0.2529\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 2.1481 - accuracy: 0.3568\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.9582 - accuracy: 0.4061\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.5265 - accuracy: 0.5124\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 1.3517 - accuracy: 0.5778\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 1.1006 - accuracy: 0.6543\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.8147 - accuracy: 0.7569\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.6553 - accuracy: 0.8005\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.5184 - accuracy: 0.8454\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4061 - accuracy: 0.8796\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.3042 - accuracy: 0.9199\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.2470 - accuracy: 0.9393\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.2101 - accuracy: 0.9463\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1766 - accuracy: 0.9571\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1434 - accuracy: 0.9648\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1105 - accuracy: 0.9752\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0932 - accuracy: 0.9799\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0700 - accuracy: 0.9873\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9903\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0467 - accuracy: 0.9936\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0384 - accuracy: 0.9946\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0321 - accuracy: 0.9950\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0265 - accuracy: 0.9963\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0214 - accuracy: 0.9977\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0185 - accuracy: 0.9983\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0155 - accuracy: 0.9997\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0143 - accuracy: 0.9993\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0118 - accuracy: 0.9997\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23ec6d10670>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is cumulative!\n",
    "model.fit(train_dataset, epochs=100, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99144197e-01, 1.23168010e-13, 4.14843550e-12, ...,\n",
       "        8.81242656e-19, 8.17872597e-18, 2.00098160e-08],\n",
       "       [9.98609543e-01, 2.43779329e-19, 3.74297591e-14, ...,\n",
       "        6.21687875e-22, 1.25255135e-15, 6.77315951e-13],\n",
       "       [9.90125537e-01, 1.24627793e-12, 2.49059717e-10, ...,\n",
       "        1.96514423e-16, 2.01126702e-12, 1.45776381e-12],\n",
       "       ...,\n",
       "       [3.50427860e-03, 1.26822455e-10, 7.34808229e-16, ...,\n",
       "        1.14208073e-16, 3.98528551e-21, 8.77774477e-01],\n",
       "       [8.19863857e-08, 8.04696441e-01, 6.15544850e-04, ...,\n",
       "        5.70216596e-10, 3.23329419e-10, 3.15211131e-03],\n",
       "       [3.60511482e-10, 5.09527012e-17, 1.16922217e-18, ...,\n",
       "        4.24508933e-27, 1.02122036e-23, 9.99998689e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 0.1639 - accuracy: 0.9664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16388532519340515, 0.9663536548614502]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "110ce578b8677349b8472a66146755bd1a67b699d95d0193dbf21e135c34359f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
