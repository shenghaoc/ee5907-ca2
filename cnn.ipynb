{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shenghaoc/ee5907-ca2/blob/main/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "NUM_SUBJECTS = 68\n",
    "NUM_CHOSEN = 25\n",
    "NUM_IMAGES_PER_SUBJECT = 170\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "NUM_IMAGES = NUM_CHOSEN * NUM_IMAGES_PER_SUBJECT\n",
    "NUM_TRAIN_IMAGES_PER_SUBJECT = np.int_(np.around(TRAIN_RATIO * NUM_IMAGES_PER_SUBJECT))\n",
    "NUM_TRAIN_IMAGES = NUM_CHOSEN * NUM_TRAIN_IMAGES_PER_SUBJECT\n",
    "NUM_TEST_IMAGES = NUM_IMAGES - NUM_TRAIN_IMAGES\n",
    "\n",
    "NUM_SELFIES = 10\n",
    "NUM_TRAIN_SELFIES = np.int_(np.around(TRAIN_RATIO * NUM_SELFIES))\n",
    "NUM_TEST_SELFIES = NUM_SELFIES - NUM_TRAIN_SELFIES\n",
    "SELFIE_LABEL = NUM_SUBJECTS + 1\n",
    "\n",
    "NUM_TOTAL_TRAIN_IMAGES = NUM_TRAIN_IMAGES + NUM_TRAIN_SELFIES\n",
    "NUM_TOTAL_TEST_IMAGES = NUM_TEST_IMAGES + NUM_TEST_SELFIES\n",
    "\n",
    "SEED1 = 2021\n",
    "SEED2 = 2022\n",
    "\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "NUM_PIXELS = WIDTH * HEIGHT\n",
    "\n",
    "# New constants due to need to fit input for tensorflow\n",
    "NUM_PEOPLE = NUM_CHOSEN + 1  # meaning plus the person with 10 selfies\n",
    "NUM_CHANNELS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure that the directory to store figures is created\n",
    "figures_directory = Path(\"report\") / \"figures\"\n",
    "figures_directory.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Must start from 1 to accommodate folder naming scheme\n",
    "# Choose NUM_CHOSEN elements from NUM_SUBJECTS integers without replacement\n",
    "chosen = np.random.default_rng(SEED1).choice(\n",
    "    np.arange(1, NUM_SUBJECTS + 1), NUM_CHOSEN, replace=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load images from disk\n",
    "# Use lists for manual looping without use of numpy functions\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Assume PIE is in pwd\n",
    "directory = Path(\"PIE\")\n",
    "for i in range(len(chosen)):\n",
    "    # Do not flatten yet, need to split train and test for each subject\n",
    "    subject_images = []\n",
    "    subject_labels = []\n",
    "    subdirectory = directory / str(chosen[i])\n",
    "    # Order is arbitrary for glob, but better to shuffle anyway\n",
    "    files = list(subdirectory.glob(\"*.jpg\"))\n",
    "    np.random.default_rng(SEED2).shuffle(files)\n",
    "    for filename in files:\n",
    "        # PIL is slower but OpenCV is unnecessary\n",
    "        im = Image.open(filename)\n",
    "        subject_images.append(np.array(im))\n",
    "        # For tensorflow input, use sequential label\n",
    "        subject_labels.append(i)\n",
    "    images.append(subject_images)\n",
    "    labels.append(subject_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Slightly altered code for selfies\n",
    "selfie_images = []\n",
    "selfie_labels = []\n",
    "\n",
    "directory = Path(\"resized\")\n",
    "# Assume selfies have been resized and folder is in pwd\n",
    "for filename in directory.glob(\"*.jpg\"):\n",
    "    im = Image.open(filename)\n",
    "    selfie_images.append(np.array(im))\n",
    "    # For tensorflow input, use number of chosen subjects (25) to avoid clashes\n",
    "    selfie_labels.append(NUM_CHOSEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Further processing without disk access\n",
    "# Train-test split\n",
    "images_train, images_test = np.split(\n",
    "    np.array(images), [NUM_TRAIN_IMAGES_PER_SUBJECT], axis=1\n",
    ")\n",
    "labels_train, labels_test = np.split(\n",
    "    np.array(labels), [NUM_TRAIN_IMAGES_PER_SUBJECT], axis=1\n",
    ")\n",
    "\n",
    "selfie_images_train, selfie_images_test = np.split(\n",
    "    np.array(selfie_images), [NUM_TRAIN_SELFIES]\n",
    ")\n",
    "selfie_labels_train, selfie_labels_test = np.split(\n",
    "    np.array(selfie_labels), [NUM_TRAIN_SELFIES]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Flatterning\n",
    "# For Conv2D, a 4+D tensor is required, add 1 for the grayscale channel\n",
    "images_train = images_train.reshape(NUM_TRAIN_IMAGES, WIDTH, HEIGHT, NUM_CHANNELS)\n",
    "selfie_images_train = selfie_images_train.reshape(\n",
    "    NUM_TRAIN_SELFIES, WIDTH, HEIGHT, NUM_CHANNELS\n",
    ")\n",
    "images_test = images_test.reshape(NUM_TEST_IMAGES, WIDTH, HEIGHT, NUM_CHANNELS)\n",
    "selfie_images_test = selfie_images_test.reshape(\n",
    "    NUM_TEST_SELFIES, WIDTH, HEIGHT, NUM_CHANNELS\n",
    ")\n",
    "\n",
    "labels_train = labels_train.reshape(NUM_TRAIN_IMAGES)\n",
    "labels_test = labels_test.reshape(NUM_TEST_IMAGES)\n",
    "\n",
    "# Combine PIE images and selfies\n",
    "total_images_train = np.append(\n",
    "    images_train,\n",
    "    selfie_images_train,\n",
    "    axis=0,\n",
    ")\n",
    "total_labels_train = np.append(labels_train, selfie_labels_train)\n",
    "\n",
    "total_images_test = np.append(\n",
    "    images_test,\n",
    "    selfie_images_test,\n",
    "    axis=0,\n",
    ")\n",
    "total_labels_test = np.append(labels_test, selfie_labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start of CNN code\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "CONV_KERNEL_SIZE = 5\n",
    "MAX_POOL_KERNEL_SIZE = 2\n",
    "MAX_POOL_SIZE = 2\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numpy arrays\n",
    "# Use built-in one-hot encoder, the numerical labels have no meaning, encoding is necessary to avoid misinterpretation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (total_images_train, tf.keras.utils.to_categorical(total_labels_train))\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (total_images_test, tf.keras.utils.to_categorical(total_labels_test))\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset.cache()\n",
    "    .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=AUTOTUNE)\n",
    ")\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        # Not really necessary, but good practice?\n",
    "        tf.keras.layers.Rescaling(1.0 / 255, input_shape=(WIDTH, HEIGHT, NUM_CHANNELS)),\n",
    "        tf.keras.layers.Conv2D(20, CONV_KERNEL_SIZE, activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(MAX_POOL_KERNEL_SIZE, MAX_POOL_KERNEL_SIZE),\n",
    "            strides=(MAX_POOL_SIZE, MAX_POOL_SIZE),\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(50, CONV_KERNEL_SIZE, activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(\n",
    "            pool_size=(MAX_POOL_KERNEL_SIZE, MAX_POOL_KERNEL_SIZE),\n",
    "            strides=(MAX_POOL_SIZE, MAX_POOL_SIZE),\n",
    "        ),\n",
    "        tf.keras.layers.Flatten(),  # too many dimensions after Conv2D\n",
    "        tf.keras.layers.Dense(500, activation=\"relu\"),\n",
    "        # Keras documentation: often used for last layer because result can be interpreted as \n",
    "        # a probability distribution \n",
    "        tf.keras.layers.Dense(NUM_PEOPLE, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(amsgrad=True), # newest ADAM\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), # multi-class labeling\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 20)        520       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 20)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 50)        25050     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1250)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               625500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                13026     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 664,096\n",
      "Trainable params: 664,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 3s 8ms/step - loss: 3.3149 - accuracy: 0.0124\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 3.2604 - accuracy: 0.0399\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2583 - accuracy: 0.0399\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2594 - accuracy: 0.0402\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2563 - accuracy: 0.0195\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2553 - accuracy: 0.0137\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2537 - accuracy: 0.0188\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2458 - accuracy: 0.0275\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2161 - accuracy: 0.0228\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2499 - accuracy: 0.0168\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2182 - accuracy: 0.0540\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2560 - accuracy: 0.0456\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.2425 - accuracy: 0.0620\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.1923 - accuracy: 0.0359\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.1175 - accuracy: 0.0684\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.0670 - accuracy: 0.0570\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.0356 - accuracy: 0.0711\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.9645 - accuracy: 0.0741\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.9584 - accuracy: 0.0989\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 3.0300 - accuracy: 0.0701\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2.9327 - accuracy: 0.0942\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 2.8139 - accuracy: 0.1291\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.6924 - accuracy: 0.1667\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.6145 - accuracy: 0.1999\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.5077 - accuracy: 0.2465\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.3749 - accuracy: 0.2646\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.2222 - accuracy: 0.3125\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 2.0309 - accuracy: 0.3773\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1.8500 - accuracy: 0.4453\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1.6362 - accuracy: 0.5034\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1.4298 - accuracy: 0.5986\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1.2633 - accuracy: 0.6415\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 1.0542 - accuracy: 0.7002\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.9098 - accuracy: 0.7374\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7407 - accuracy: 0.7928\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.8038\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.5305 - accuracy: 0.8508\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.4177 - accuracy: 0.8823\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.8987\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.2844 - accuracy: 0.9219\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1936 - accuracy: 0.9480\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1465 - accuracy: 0.9621\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9769\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9863\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9916\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0470 - accuracy: 0.9943\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0385 - accuracy: 0.9963\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9963\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 0.9980\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.9987\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 0.9997\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9997\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9997\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 9.7419e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 9.3977e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 9.0221e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 8.6757e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 8.3236e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 8.0312e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 7.7093e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 7.4529e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 0s 4ms/step - loss: 7.1701e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219035aa550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is cumulative!\n",
    "model.fit(train_dataset, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9877876e-01, 3.9071232e-11, 4.0966822e-12, ..., 6.5996674e-14,\n",
       "        3.4669119e-18, 3.2137787e-25],\n",
       "       [9.9992216e-01, 1.4903825e-19, 4.6994630e-10, ..., 2.8153952e-20,\n",
       "        4.1978117e-18, 3.8841161e-12],\n",
       "       [9.9547750e-01, 2.0711067e-16, 1.1433006e-12, ..., 4.4955522e-16,\n",
       "        1.3253882e-18, 9.6527593e-18],\n",
       "       ...,\n",
       "       [1.3278513e-11, 1.4172599e-06, 2.2658152e-03, ..., 3.8847380e-12,\n",
       "        9.9752563e-01, 3.9803616e-15],\n",
       "       [2.7078653e-10, 3.6637538e-15, 1.0135809e-09, ..., 2.0191246e-13,\n",
       "        1.9771134e-12, 9.9977070e-01],\n",
       "       [2.7198415e-19, 1.2415699e-12, 7.5611839e-10, ..., 1.6005873e-23,\n",
       "        9.9980897e-01, 1.9282397e-24]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1606 - accuracy: 0.9718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16055940091609955, 0.9718309640884399]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "110ce578b8677349b8472a66146755bd1a67b699d95d0193dbf21e135c34359f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
